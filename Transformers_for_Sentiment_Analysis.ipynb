{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Transformers for Sentiment Analysis\n",
    "\n",
    "In this notebook we will be using the transformer model, first introduced in [this](https://arxiv.org/abs/1706.03762) paper. Specifically, we will be using the BERT (Bidirectional Encoder Representations from Transformers) model from [this](https://arxiv.org/abs/1810.04805) paper. \n",
    "\n",
    "Transformer models are considerably larger than anything else covered in these tutorials. As such we are going to use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as our embedding layers. We will freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer. In this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "First, as always, let's set the random seeds for deterministic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n",
    "\n",
    "Luckily, the transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained `bert-base-uncased` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers 라이브러리에서 BertTokenizer를 가져옴\n",
    "from transformers import BertTokenizer\n",
    "# bert-base-uncased 라는 pretrained된 BERT 모델의 tokenizer를 불러옴\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenizer` has a `vocab` attribute which contains the actual vocabulary we will be using. We can check how many tokens are in it by checking its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer의 어휘 사진(vocab)의 크기를 출력함\n",
    "# 이는 BERT의 pretrain 시 사용된 고유한 token(단어)의 수를 나타냄\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tokenizer is as simple as calling `tokenizer.tokenize` on a string. This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "#토크나이저를 사용해 토큰화한 후, 토큰화된 결과 출력\n",
    "#특징: 소문자로 변환, 물음표 구분\n",
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can numericalize tokens using our vocabulary using `tokenizer.convert_tokens_to_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "#토크나이저의 어휘 사전을 사용하여 해당 토큰들의 고유한 ID로 변환하는 작업을 수행함\n",
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer was also trained with special tokens to mark the beginning and end of the sentence, detailed [here](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel). As well as a standard padding and unknown token. We can also get these from the tokenizer.\n",
    "\n",
    "**Note**: the tokenizer does have a beginning of sequence and end of sequence attributes (`bos_token` and `eos_token`) but these are not set and should not be used for this transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "#토크나이저에서 특별한 토큰들을 추출함\n",
    "#[CLS] 토큰을 추출합니다. 대부분의 Transformer 모델에서 문장의 시작 부분에 사용됩니다.\n",
    "# [SEP] 토큰을 추출합니다. 두 개의 문장을 구분하거나 문장의 끝을 나타내는 데 사용됩니다.\n",
    "# [PAD] 토큰을 추출합니다. 입력 시퀀스의 길이를 조정하기 위해 사용되는 패딩 토큰입니다.\n",
    "# [UNK] 토큰을 추출합니다. 어휘 사전에 없는 알 수 없는 단어를 나타내기 위한 토큰입니다.\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the indexes of the special tokens by converting them using the vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or by explicitly getting them from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we need to handle is that the model was trained on sequences with a defined maximum length - it does not know how to handle sequences longer than it has been trained on. We can get the maximum length of these input sizes by checking the `max_model_input_sizes` for the version of the transformer we want to use. In this case, it is 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델에서 허용하는 최대 입력 길이\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have used the `spaCy` tokenizer to tokenize our examples. However we now need to define a function that we will pass to our `TEXT` field that will handle all the tokenization for us. It will also cut down the number of tokens to a maximum length. Note that our maximum length is 2 less than the actual maximum length. This is because we need to append two tokens to each sequence, one to the start and one to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 문장을 토큰화하고, BERT 모델의 최대 입력 길이에 맞게 토큰을 잘라냄\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    # 토큰화된 결과를 BERT의 최대 입력 길이에 맞게 잘라냅니다.\n",
    "    # 이 때, -2를 하는 이유는 [CLS]와 [SEP] 토큰을 위한 공간을 확보하기 위함입니다.\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes.\n",
    "\n",
    "We define the label field as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "\n",
    "# TEXT는 입력 데이터를 처리하기 위한 Field 객체, 데이터의 로딩, 토큰화, 전처리 및 배치화 과정을 처리\n",
    "TEXT = data.Field(batch_first = True, # 배치 차원을 첫 번째 차원으로 설정\n",
    "                  use_vocab = False, # BERT를 사용하기 때문에 별도의 어휘 사전 구축이 필요 없음\n",
    "                  tokenize = tokenize_and_cut, # 주어진 함수를 사용하여 토큰화 함\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids, # 토큰을 ID로 변환\n",
    "                  init_token = init_token_idx, # 시작 토큰의 ID를 지정\n",
    "                  eos_token = eos_token_idx, # 종료 토큰의 ID를 지정\n",
    "                  pad_token = pad_token_idx, # 패딩 토큰의 ID를 지정\n",
    "                  unk_token = unk_token_idx) # 알 수 없은 토큰의 ID를 지정\n",
    "\n",
    "# LABEL은 라벨(주로 분류 태스크의 클래스 또는 회귀 값)를 처리하기 위한 Field 객체\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and create the validation splits as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:10<00:00, 8.15MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import datasets\n",
    "# IMDB 데이터셋을 로드\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "# train_data 를 학습,검증 데이터셋으로 구분\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check an example and ensure that the text has already been numericalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [1000, 6887, 26802, 6491, 1000, 1997, 3245, 2001, 1037, 3811, 12483, 1010, 17109, 1010, 12459, 1998, 2200, 2434, 5469, 17312, 1010, 1998, 1010, 1999, 2028, 2773, 1010, 8754, 1012, 1996, 2034, 8297, 1997, 2997, 2001, 2175, 2854, 1010, 25591, 1010, 2895, 1011, 8966, 1998, 3811, 14036, 1012, 2044, 1996, 2034, 8297, 2174, 1010, 1000, 6887, 26802, 6491, 1000, 8543, 2123, 2522, 15782, 16230, 2100, 4593, 10858, 2047, 4784, 1012, 1000, 6887, 26802, 6491, 3523, 1011, 2935, 1997, 1996, 2757, 1000, 1997, 2807, 2003, 5121, 2025, 1037, 3143, 4945, 1010, 2009, 2130, 2003, 3243, 14036, 1010, 2021, 2045, 2003, 2053, 2062, 2434, 3012, 1010, 1998, 1996, 7143, 4740, 2000, 3288, 1999, 2242, 2047, 1010, 2024, 2012, 2335, 13310, 8462, 1010, 2029, 3084, 2009, 3243, 15640, 1999, 7831, 2000, 2049, 16372, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1011, 27594, 2545, 1011, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3243, 1999, 1996, 2927, 1010, 2057, 2024, 3107, 1996, 3595, 2369, 1996, 8075, 16074, 19885, 1006, 1996, 4167, 1011, 13475, 3909, 3165, 7395, 1007, 2003, 4895, 22401, 3709, 1012, 23166, 29278, 7652, 1010, 1037, 2193, 1997, 14203, 1998, 15703, 2047, 3494, 1006, 2107, 2004, 5199, 1010, 1037, 1000, 2188, 2894, 1000, 1011, 2806, 2210, 4845, 2040, 6433, 2000, 2022, 2307, 2012, 5008, 1010, 2019, 6857, 1010, 1037, 7823, 1998, 3565, 1011, 4658, 16634, 7507, 5283, 1011, 11820, 2304, 14556, 2007, 1037, 3626, 3013, 1007, 2024, 3107, 1012, 1996, 2143, 2036, 2038, 2049, 11647, 1011, 17963, 7221, 26942, 2121, 2003, 2153, 2200, 4658, 2004, 1996, 15606, 1011, 14578, 1010, 2858, 2652, 17963, 1012, 13682, 8040, 20026, 2213, 2003, 2145, 3243, 17109, 2004, 1996, 4206, 2158, 1010, 2021, 1996, 2755, 2008, 1996, 4206, 2158, 7566, 1037, 2843, 2062, 1999, 2023, 2143, 1010, 3084, 2032, 6065, 2070, 1997, 2010, 19815, 9961, 1012, 1996, 2839, 1997, 3505, 2003, 2209, 2011, 1037, 1012, 2745, 10970, 2153, 1006, 2002, 2018, 2042, 2999, 2011, 2508, 4190, 7352, 1999, 2112, 1016, 1007, 1010, 2029, 1010, 1999, 2026, 5448, 1010, 2987, 1005, 1056, 2191, 2172, 1997, 1037, 4489, 1012, 1996, 13638, 2036, 7906, 1996, 2143, 5875, 2438, 2000, 3422, 1010, 2021, 2009, 2003, 2145, 1037, 10520, 1010, 2926, 2138, 1996, 4740, 2000, 2191, 2039, 2005, 1996, 3768, 1997, 4784, 2131, 15703, 3243, 2855, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2035, 2477, 2641, 1010, 1000, 6887, 26802, 6491, 3523, 1000, 2003, 2019, 11701, 2051, 1011, 5949, 2099, 1010, 2021, 2009, 2003, 5791, 15640, 4102, 2000, 2049, 16372, 1012, 4599, 1997, 1996, 2034, 2048, 1000, 6887, 26802, 6491, 1000, 3152, 2064, 2507, 2009, 1037, 3046, 1010, 2021, 1045, 16755, 2025, 2000, 2275, 2115, 10908, 2205, 2152, 1012], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "# train_data의 7번째 예제에 대한 정보 출력하고\n",
    "# 해당 예제의 텍스트를 토큰화된 형태로 출력\n",
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'ph', '##anta', '##sm', '\"', 'of', '1979', 'was', 'a', 'highly', 'atmospheric', ',', 'creepy', ',', 'scary', 'and', 'very', 'original', 'horror', 'flick', ',', 'and', ',', 'in', 'one', 'word', ',', 'cult', '.', 'the', 'first', 'sequel', 'of', '1988', 'was', 'go', '##ry', ',', 'witty', ',', 'action', '-', 'packed', 'and', 'highly', 'entertaining', '.', 'after', 'the', 'first', 'sequel', 'however', ',', '\"', 'ph', '##anta', '##sm', '\"', 'creator', 'don', 'co', '##sca', '##rell', '##y', 'apparently', 'lacked', 'new', 'ideas', '.', '\"', 'ph', '##anta', '##sm', 'iii', '-', 'lord', 'of', 'the', 'dead', '\"', 'of', '1994', 'is', 'certainly', 'not', 'a', 'complete', 'failure', ',', 'it', 'even', 'is', 'quite', 'entertaining', ',', 'but', 'there', 'is', 'no', 'more', 'original', '##ity', ',', 'and', 'the', 'desperate', 'attempts', 'to', 'bring', 'in', 'something', 'new', ',', 'are', 'at', 'times', 'tires', '##ome', ',', 'which', 'makes', 'it', 'quite', 'disappointing', 'in', 'comparison', 'to', 'its', 'predecessors', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', '-', 'spoil', '##ers', '-', '<', 'br', '/', '>', '<', 'br', '/', '>', 'quite', 'in', 'the', 'beginning', ',', 'we', 'are', 'introduced', 'the', 'secret', 'behind', 'the', 'mysterious', 'sentinel', 'spheres', '(', 'the', 'brain', '-', 'sucking', 'flying', 'silver', 'balls', ')', 'is', 'un', '##rave', '##led', '.', 'thence', '##for', '##ward', ',', 'a', 'number', 'of', 'unnecessary', 'and', 'annoying', 'new', 'characters', '(', 'such', 'as', 'tim', ',', 'a', '\"', 'home', 'alone', '\"', '-', 'style', 'little', 'kid', 'who', 'happens', 'to', 'be', 'great', 'at', 'shooting', ',', 'an', 'rocky', ',', 'a', 'tough', 'and', 'super', '-', 'cool', 'nun', '##cha', '##ku', '-', 'swinging', 'black', 'chick', 'with', 'a', 'crew', 'cut', ')', 'are', 'introduced', '.', 'the', 'film', 'also', 'has', 'its', 'qualities', '-', 'reggie', 'ban', '##nist', '##er', 'is', 'again', 'very', 'cool', 'as', 'the', 'pony', '-', 'tailed', ',', 'guitar', 'playing', 'reggie', '.', 'angus', 'sc', '##rim', '##m', 'is', 'still', 'quite', 'creepy', 'as', 'the', 'tall', 'man', ',', 'but', 'the', 'fact', 'that', 'the', 'tall', 'man', 'talks', 'a', 'lot', 'more', 'in', 'this', 'film', ',', 'makes', 'him', 'loose', 'some', 'of', 'his', 'creep', '##iness', '.', 'the', 'character', 'of', 'mike', 'is', 'played', 'by', 'a', '.', 'michael', 'baldwin', 'again', '(', 'he', 'had', 'been', 'replaced', 'by', 'james', 'leg', '##ros', 'in', 'part', '2', ')', ',', 'which', ',', 'in', 'my', 'opinion', ',', 'doesn', \"'\", 't', 'make', 'much', 'of', 'a', 'difference', '.', 'the', 'gore', 'also', 'keeps', 'the', 'film', 'interesting', 'enough', 'to', 'watch', ',', 'but', 'it', 'is', 'still', 'a', 'disappointment', ',', 'especially', 'because', 'the', 'attempts', 'to', 'make', 'up', 'for', 'the', 'lack', 'of', 'ideas', 'get', 'annoying', 'quite', 'quickly', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'all', 'things', 'considered', ',', '\"', 'ph', '##anta', '##sm', 'iii', '\"', 'is', 'an', 'acceptable', 'time', '-', 'waste', '##r', ',', 'but', 'it', 'is', 'definitely', 'disappointing', 'compared', 'to', 'its', 'predecessors', '.', 'fans', 'of', 'the', 'first', 'two', '\"', 'ph', '##anta', '##sm', '\"', 'films', 'can', 'give', 'it', 'a', 'try', ',', 'but', 'i', 'recommend', 'not', 'to', 'set', 'your', 'expectations', 'too', 'high', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위에서 출력한 예제의 'text' key의 value을 BERT 토크나이저를 사용하여 토큰화된 형태로 변환\n",
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'pos'과 'neg'을 고유한 정수 ID로 매핑\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we create the iterators. Ideally we want to use the largest batch size that we can as I've found this gives the best results for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#data.BucketIterator를 사용하여 데이터를 배치 단위로 로드하기 위한 iterator를 생성\n",
    "#BucketIterator는 유사한 길이를 가진 예제들을 함꼐 배치를 만드는 것을 목표로 함\n",
    "#패딩의 양을 최소화하여 계산 효율성이 향상됨\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Next, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [00:13<00:00, 33.8MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 'bert-base-uncased'는 BERT의 기본 버전을 나타냅니다. \n",
    "# 이 모델은 소문자만 사용하여 학습되었으며, 원본 BERT의 중간 크기에 해당합니다.\n",
    "# from_pretrained 메소드를 사용하여 해당 모델을 로드합니다.\n",
    "# 이렇게 하면 사전 학습된 가중치를 포함한 BERT 모델이 생성됩니다.\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our actual model. \n",
    "\n",
    "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
    "\n",
    "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# BERT 모델과 GRU(Gated Recurrent Unit)를 결합하여 감정 분석을 위한 BERTGRUSentiment 모델을 정의합니다.\n",
    "# 이 모델은 먼저 BERT를 사용하여 문장을 임베딩하고, 결과를 GRU에 전달합니다. 마지막으로, GRU의 최종 hidden state는 분류 작업을 위해 Linear layer에 전달됩니다.\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        # BERT 모델의 embedding 차원을 가져옵니다.\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        # GRU layer를 정의합니다.\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        # 최종 분류를 위한 Linear layer를 정의합니다.\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an instance of our model using standard hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 하이퍼파라미터를 정의합니다.\n",
    "HIDDEN_DIM = 256 # GRU의 hidden state 차원\n",
    "OUTPUT_DIM = 1  # 출력 차원 (여기서는 긍정/부정 분류를 위한 1차원 출력)\n",
    "N_LAYERS = 2 # GRU의 레이어 수\n",
    "BIDIRECTIONAL = True  # 양방향 GRU를 사용\n",
    "DROPOUT = 0.25 # 드롭아웃 비율\n",
    "\n",
    "# `BERTGRUSentiment` 모델을 초기화합니다. \n",
    "# 위에서 정의한 하이퍼파라미터와 함께 사전 학습된 BERT 모델을 전달합니다.\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 주어진 모델의 학습 가능한 파라미터의 총 수를 반환하는 함수를 정의합니다.\n",
    "def count_parameters(model):\n",
    "    # model.parameters()는 모델의 모든 파라미터를 반환합니다.\n",
    "    # p.requires_grad는 해당 파라미터가 학습 중에 업데이트되어야 하는지 여부를 나타냅니다.\n",
    "    # p.numel()은 파라미터의 원소 수를 반환합니다.\n",
    "    # 따라서, 이 함수는 모든 학습 가능한 파라미터의 원소 수를 합산합니다.\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델과 관련된 파라미터는 학습에 업데이트하지 않음\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,759,169 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters of the GRU (`rnn`) and the linear layer (`out`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "As is standard, we define our optimizer and criterion (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진분류 작업에 적합한 손실 함수 BCEWithLogitsLoss 사용, 시그모이드 함수\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the model and criterion onto the GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    # 모델의 출력인 preds는 로짓값이므로, 시그모이드를 통과하여 확률값을 얻는다.\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # 모든 파라미터의 기울기를 0으로 초기화합니다. \n",
    "        # 이는 PyTorch에서는 같은 파라미터에 대해 여러 번의 backward() 호출로 기울기가 누적되기 때문에 필요합니다.\n",
    "        optimizer.zero_grad()\n",
    "        # 모델을 사용하여 현재 배치의 예측값을 계산합니다.\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        # 예측값과 실제 레이블을 사용하여 손실을 계산합니다.\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        # 예측값과 실제 레이블을 사용하여 정확도를 계산합니다.\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        # 손실에 대한 기울기를 계산합니다\n",
    "        loss.backward()\n",
    "        # optimizer를 사용하여 모델의 파라미터를 업데이트합니다.\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    # 모델을 평가 모드로 설정합니다.\n",
    "    # 이렇게 하면 모델 내의 dropout, batch normalization 등의 레이어가 evaluation 모드로 동작합니다.\n",
    "    model.eval()\n",
    "    # 기울기 계산이 필요하지 않기 때문에 torch.no_grad() 내에서 연산을 수행합니다.\n",
    "    # 이렇게 함으로써 메모리 사용량을 줄이고 속도를 높일 수 있습니다.\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            # 모델을 사용하여 현재 배치의 예측값을 계산합니다.\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            # 예측값과 실제 레이블을 사용하여 손실을 계산합니다.\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            # 예측값과 실제 레이블을 사용하여 정확도를 계산합니다.\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 소요 시간 계산\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll train our model. This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 6m 44s\n",
      "\tTrain Loss: 0.535 | Train Acc: 72.30%\n",
      "\t Val. Loss: 0.294 |  Val. Acc: 88.04%\n",
      "Epoch: 02 | Epoch Time: 6m 48s\n",
      "\tTrain Loss: 0.322 | Train Acc: 86.64%\n",
      "\t Val. Loss: 0.240 |  Val. Acc: 90.73%\n",
      "Epoch: 03 | Epoch Time: 6m 49s\n",
      "\tTrain Loss: 0.272 | Train Acc: 88.83%\n",
      "\t Val. Loss: 0.235 |  Val. Acc: 90.68%\n",
      "Epoch: 04 | Epoch Time: 6m 49s\n",
      "\tTrain Loss: 0.238 | Train Acc: 90.40%\n",
      "\t Val. Loss: 0.227 |  Val. Acc: 91.06%\n",
      "Epoch: 05 | Epoch Time: 6m 49s\n",
      "\tTrain Loss: 0.209 | Train Acc: 91.80%\n",
      "\t Val. Loss: 0.219 |  Val. Acc: 91.09%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # 학습과 검증 시행\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    # 모델 저장\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.211 | Test Acc: 91.51%\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델의 파라미터 로드\n",
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "# 성능 평가\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인퍼런스\n",
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    #주어진 문장을 토크나이즈하여 토큰 리스트를 반환합니다.\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    #문장의 길이가 최대 입력 길이를 초과하는 경우, 최대 입력 길이만큼만 토큰을 가져옵니다. -2는 시작 토큰과 종료 토큰을 위한 공간을 확보하기 위함입니다.\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    # 토큰들을 인덱스로 변환하고, 시작 토큰과 종료 토큰을 추가합니다.\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    #인덱스 리스트를 텐서로 변환하고, 적절한 디바이스(CPU 또는 GPU)로 이동시킵니다.\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    #배치 차원을 추가합니다. 모델은 배치 입력을 기대하기 때문입니다.\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    #모델을 사용하여 텐서에 대한 예측값을 계산하고, 시그모이드 함수를 적용하여 [0, 1] 범위의 값을 얻습니다\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    #예측값을 Python의 float 값으로 반환합니다.\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05781012400984764"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956889271736145"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
